{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\ndef compute(X, y, theta):\n    m = len(y)\n    gradient = np.dot(X.T, (np.dot(X, theta) - y)) / m\n    return gradient\n\ndef gradient_descent(X, y, theta_initial, learning_rate, num_iterations):\n    \n    theta = theta_initial\n    m = len(y)\n    cost_history = []\n    \n    for _ in range(num_iterations):\n        gradient = compute_gradient(X, y, theta)\n        theta -= learning_rate * gradient\n        cost = np.sum((np.dot(X, theta) - y) ** 2) / (2 * m)\n        cost_history.append(cost)\n        \n    return theta, cost_history\n\nnp.random.seed(0)\nm = 100\nn = 2\nX = np.random.rand(m, n)\nX = np.hstack((np.ones((m, 1)), X))  # Adding intercept term\ntheta_true = np.random.rand(n + 1)\ny = np.dot(X, theta_true) + np.random.randn(m) * 0.1\n\n# Initial parameters and hyperparameters\ntheta_initial = np.zeros(n + 1)\nlearning_rate = 0.01\nnum_iterations = 1000\n\n#Gradient descent\ntheta_optimized, cost_history = gradient_descent(X, y, theta_initial, learning_rate, num_iterations)\n\nprint(\"Optimized parameters are=\", theta_optimized)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Optimized parameters are= [0.45678283 0.47650259 0.29712789]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Loading data from CSV\ndata = pd.read_csv('univariate_linear_regression.csv')\nX = data['Input Feature']\nY = data['Target']\n\n# Define utility functions\ndef predict(x, b0, b1):\n    return b0 + b1 * x\n\ndef cost(x, y, b0, b1):\n    errors = [(predict(xi, b0, b1) - yi) for xi, yi in zip(x, y)]\n    mse = np.mean(np.square(errors))\n    return mse\n\n# Gradient Descent\ndef gradient_descent(x, y, b0, b1, learning_rate, num_iterations):\n    for _ in range(num_iterations):\n        b0_gradient = np.mean(predict(x, b0, b1) - y)\n        b1_gradient = np.mean((predict(x, b0, b1) - y) * x)\n        b0 -= learning_rate * b0_gradient\n        b1 -= learning_rate * b1_gradient\n    return b0, b1\n\n# Initializing coefficients\nlearning_rate = 0.01\nnum_iterations = 1000\nb0_initial, b1_initial = 0, 0\n\n# Training the model\nb0_final, b1_final = gradient_descent(X, Y, b0_initial, b1_initial, learning_rate, num_iterations)\n\n# Plotting the cost function\nb0_vals = np.linspace(-10, 10, 100)\nb1_vals = np.linspace(-1, 1, 100)\nB0, B1 = np.meshgrid(b0_vals, b1_vals)\ncost_vals = np.array([cost(X, Y, b0, b1) for b0, b1 in zip(B0.ravel(), B1.ravel())]).reshape(B0.shape)\n\nfig = plt.figure(figsize=(12, 6))\nax1 = fig.add_subplot(121, projection='3d')\nax1.plot_surface(B0, B1, cost_vals, cmap='viridis')\nax1.set_xlabel('b0')\nax1.set_ylabel('b1')\nax1.set_zlabel('Cost Function')\nax1.set_title('Cost Function Surface')\n\nax2 = fig.add_subplot(122)\ncontour = ax2.contour(B0, B1, cost_vals, levels=20, cmap='viridis')\nax2.set_xlabel('b0')\nax2.set_ylabel('b1')\nax2.set_title('Cost Function Contour')\n\n# Plot the best fit line\nplt.figure(figsize=(8, 6))\nplt.scatter(X, Y, label='Data Points')\nplt.plot(X, predict(X, b0_final, b1_final), color='red', label='Best Fit Line')\nplt.xlabel('Input Feature (X)')\nplt.ylabel('Target (Y)')\nplt.title('Univariate Linear Regression')\nplt.legend()\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.linear_model import LinearRegression\n\ndata = pd.read_csv('heart.data.csv') # Loading data from CSV\n\nX = data[['biking', 'smoking']]  #independent variables\nY = data['heart.disease']  #dependent variable\n\nmodel = LinearRegression() # Initializing and training the linear regression model\nmodel.fit(X, Y)\n\n# Coefficients for each feature\nb0 = model.intercept_\nb1, b2 = model.coef_\n\n# Creating a grid for plotting the plane\nbiking_vals = np.linspace(min(X['biking']), max(X['biking']), 50)\nsmoking_vals = np.linspace(min(X['smoking']), max(X['smoking']), 50)\nBiking, Smoking = np.meshgrid(biking_vals, smoking_vals)\nHeart_Disease_Predicted = b0 + b1 * Biking + b2 * Smoking\n\n# Plotting the best fit plane\nfig = plt.figure(figsize=(10, 6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X['biking'], X['smoking'], Y, c='b', marker='o', label='Data Points')\nax.plot_surface(Biking, Smoking, Heart_Disease_Predicted, alpha=0.5, cmap='plasma')\nax.set_xlabel('Biking')\nax.set_ylabel('Smoking')\nax.set_zlabel('Heart Disease')\nax.set_title('Multivariate Linear Regression')\nplt.legend()\nplt.show()\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Interpretation:\n# b1: (for biking) represents the change in heart disease risk per unit increase in biking.\n# b2: (for smoking) represents the change in heart disease risk per unit increase in smoking.\n#  A positive b1 indicates that more biking is associated with higher heart disease risk.\n# -A positive b2 indicates that more smoking is associated with higher heart disease risk\n",
      "metadata": {}
    }
  ]
}